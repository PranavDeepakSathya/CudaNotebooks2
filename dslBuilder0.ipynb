{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63550ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cuda import cuda\n",
    "\n",
    "def check_driver():\n",
    "    # 1. Initialize the Driver\n",
    "    err, = cuda.cuInit(0)\n",
    "    if err != cuda.CUresult.CUDA_SUCCESS:\n",
    "        print(f\"âŒ cuInit Failed: {err}\")\n",
    "        return\n",
    "\n",
    "    # 2. Get Device Count\n",
    "    err, count = cuda.cuDeviceGetCount()\n",
    "    if err != cuda.CUresult.CUDA_SUCCESS:\n",
    "        print(f\"âŒ GetCount Failed: {err}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"ðŸŸ¢ Driver Initialized. Found {count} Device(s).\")\n",
    "\n",
    "    # 3. Get Device Name\n",
    "    for i in range(count):\n",
    "        err, device = cuda.cuDeviceGet(i)\n",
    "        err, name = cuda.cuDeviceGetName(128, device)\n",
    "        # name is bytes, decode it\n",
    "        print(f\"   GPU {i}: {name.decode('utf-8').strip()}\")\n",
    "        \n",
    "        # Check Compute Capability (Crucial for 5070/Blackwell)\n",
    "        err, major = cuda.cuDeviceGetAttribute(\n",
    "            cuda.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MAJOR, device\n",
    "        )\n",
    "        err, minor = cuda.cuDeviceGetAttribute(\n",
    "            cuda.CUdevice_attribute.CU_DEVICE_ATTRIBUTE_COMPUTE_CAPABILITY_MINOR, device\n",
    "        )\n",
    "        print(f\"          Compute Capability: {major}.{minor}\")\n",
    "\n",
    "check_driver()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e62b67",
   "metadata": {},
   "source": [
    "##### Warpgroup\n",
    "\n",
    "there exists a map $w: [0, T) -> [0, \\lfloor {T/32} \\rfloor)$ given by $t \\mapsto \\text {floor} (t/32)$\n",
    "\n",
    "indeed $w(t)$ is the warp rank of the thread t. A warpgroup is 4 contigous warps, with the warp_rank of the first warp being a multiple of 4. \n",
    "\n",
    "This is exactly what it means, threads 0,1,...31 (w0), 32,33,,,63 (w1), 64 .... 95 (w2), \n",
    "\n",
    "96 ..... 128 (w3) \n",
    "is a wrap group. its the 0th warp group. \n",
    "\n",
    "The first question we ask is that if the tensor core unit is seperate, and we fire out async WGMMA operations,\n",
    "why do we need so many threads? the answer is simple, the accumulator tile is too big to be held by the \n",
    "registers of one thread, and is instead split as fragments across the threads of a warpgroup, also the SM can paralelly issue exactly 4 warps of instructions at once, while it can concurrently issue upto 2048/32 wrarps. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a00f30",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7839f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "okay now lets look into the whole situation \n",
    "\n",
    "First the TMA, imagine we have a tensor map, descibing the shared memory and global memory layouts of our tensor\n",
    "along with any swizzle or interleave we wanna do, abstract that as T_map \n",
    "\n",
    "The first thing we need to do, is to init a barrier on shared memory with the number of arriving threads, \n",
    "and then call the tma fency proxy \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2da6173",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warp_mma_atom(c_frag, a_frag, b_frag): \n",
    "  c_frag = c_frag + a_frag@b_frag\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec736da",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_frag = np.zeros()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
