//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-36006120
// Cuda compilation tools, release 12.9, V12.9.83
// Based on NVVM 20.0.0
//

.version 8.8
.target sm_100a
.address_size 64

	// .globl	kernel_cutlass_kernel_with_prefetch_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12825616_TVLayoutA1128161281128_TVLayoutB1256162561256_TVLayoutC11282561281128_0
.extern .shared .align 1024 .b8 __dynamic_shmem__0[];

.visible .entry kernel_cutlass_kernel_with_prefetch_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12825616_TVLayoutA1128161281128_TVLayoutB1256162561256_TVLayoutC11282561281128_0(
	.param .align 1 .b8 kernel_cutlass_kernel_with_prefetch_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12825616_TVLayoutA1128161281128_TVLayoutB1256162561256_TVLayoutC11282561281128_0_param_0[3],
	.param .align 64 .b8 kernel_cutlass_kernel_with_prefetch_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12825616_TVLayoutA1128161281128_TVLayoutB1256162561256_TVLayoutC11282561281128_0_param_1[128],
	.param .align 4 .b8 kernel_cutlass_kernel_with_prefetch_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12825616_TVLayoutA1128161281128_TVLayoutB1256162561256_TVLayoutC11282561281128_0_param_2[8],
	.param .align 64 .b8 kernel_cutlass_kernel_with_prefetch_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12825616_TVLayoutA1128161281128_TVLayoutB1256162561256_TVLayoutC11282561281128_0_param_3[128],
	.param .align 4 .b8 kernel_cutlass_kernel_with_prefetch_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12825616_TVLayoutA1128161281128_TVLayoutB1256162561256_TVLayoutC11282561281128_0_param_4[8],
	.param .align 8 .b8 kernel_cutlass_kernel_with_prefetch_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12825616_TVLayoutA1128161281128_TVLayoutB1256162561256_TVLayoutC11282561281128_0_param_5[24]
)
.reqntid 128, 1, 1
{
	.reg .pred 	%p<55>;
	.reg .b16 	%rs<71>;
	.reg .b32 	%r<273>;
	.reg .f32 	%f<65>;
	.reg .b64 	%rd<44>;

	ld.param.u8 	%rs1, [kernel_cutlass_kernel_with_prefetch_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12825616_TVLayoutA1128161281128_TVLayoutB1256162561256_TVLayoutC11282561281128_0_param_0+2];
	and.b16  	%rs2, %rs1, 1;
	setp.eq.b16 	%p12, %rs2, 1;
	ld.param.u8 	%rs3, [kernel_cutlass_kernel_with_prefetch_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12825616_TVLayoutA1128161281128_TVLayoutB1256162561256_TVLayoutC11282561281128_0_param_0+1];
	and.b16  	%rs4, %rs3, 1;
	setp.eq.b16 	%p11, %rs4, 1;
	ld.param.u8 	%rs5, [kernel_cutlass_kernel_with_prefetch_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12825616_TVLayoutA1128161281128_TVLayoutB1256162561256_TVLayoutC11282561281128_0_param_0];
	and.b16  	%rs6, %rs5, 1;
	setp.eq.b16 	%p54, %rs6, 1;
	ld.param.u64 	%rd8, [kernel_cutlass_kernel_with_prefetch_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12825616_TVLayoutA1128161281128_TVLayoutB1256162561256_TVLayoutC11282561281128_0_param_5+16];
	ld.param.u64 	%rd7, [kernel_cutlass_kernel_with_prefetch_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12825616_TVLayoutA1128161281128_TVLayoutB1256162561256_TVLayoutC11282561281128_0_param_5];
	ld.param.u32 	%r69, [kernel_cutlass_kernel_with_prefetch_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12825616_TVLayoutA1128161281128_TVLayoutB1256162561256_TVLayoutC11282561281128_0_param_2+4];
	mov.b64 	%rd9, kernel_cutlass_kernel_with_prefetch_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12825616_TVLayoutA1128161281128_TVLayoutB1256162561256_TVLayoutC11282561281128_0_param_1;
	mov.b64 	%rd10, kernel_cutlass_kernel_with_prefetch_TiledMMA_ThrLayoutVMNK11110000_PermutationMNK____MMAAtom_ThrID10_ShapeMNK12825616_TVLayoutA1128161281128_TVLayoutB1256162561256_TVLayoutC11282561281128_0_param_3;
	mov.u32 	%r1, %tid.x;
	mov.u32 	%r72, %tid.y;
	mov.u32 	%r73, %tid.z;
	mov.u32 	%r74, %ntid.x;
	mov.u32 	%r75, %ntid.y;
	mad.lo.s32 	%r76, %r75, %r73, %r72;
	mad.lo.s32 	%r77, %r76, %r74, %r1;
	shr.u32 	%r78, %r77, 5;
	shfl.sync.idx.b32	%r2, %r78, 0, 31, -1;
	mov.u32 	%r3, %ctaid.y;
	mov.u32 	%r79, __dynamic_shmem__0;
	add.s32 	%r80, %r79, 215;
	and.b32  	%r4, %r80, -896;
	setp.ne.s32 	%p13, %r2, 0;
	@%p13 bra 	$L__BB0_2;
	add.s32 	%r82, %r79, 80;
	mov.b32 	%r83, 512;
	tcgen05.alloc.cta_group::1.sync.aligned.shared::cta.b32 [%r82], %r83;
	cvta.param.u64 	%rd11, %rd9;
	// begin inline asm
	prefetch.tensormap [%rd11];
	// end inline asm
	cvta.param.u64 	%rd12, %rd10;
	// begin inline asm
	prefetch.tensormap [%rd12];
	// end inline asm
	mov.b32 	%r84, 1;
	mbarrier.init.shared.b64 [%r79], %r84;
	add.s32 	%r85, %r79, 8;
	mbarrier.init.shared.b64 [%r85], %r84;
	add.s32 	%r86, %r79, 16;
	mbarrier.init.shared.b64 [%r86], %r84;
	add.s32 	%r87, %r79, 24;
	mbarrier.init.shared.b64 [%r87], %r84;
	add.s32 	%r88, %r79, 32;
	mbarrier.init.shared.b64 [%r88], %r84;
	add.s32 	%r89, %r79, 40;
	mbarrier.init.shared.b64 [%r89], %r84;
	add.s32 	%r90, %r79, 48;
	mbarrier.init.shared.b64 [%r90], %r84;
	add.s32 	%r91, %r79, 56;
	mbarrier.init.shared.b64 [%r91], %r84;
$L__BB0_2:
	setp.ne.s32 	%p14, %r2, 0;
	// begin inline asm
	fence.mbarrier_init.release.cluster;
	// end inline asm
	bar.sync 	0;
	@%p14 bra 	$L__BB0_4;
	add.s32 	%r93, %r79, 64;
	mov.b32 	%r94, 1;
	mbarrier.init.shared.b64 [%r93], %r94;
	add.s32 	%r95, %r79, 72;
	mov.b32 	%r96, 128;
	mbarrier.init.shared.b64 [%r95], %r96;
$L__BB0_4:
	mov.u32 	%r97, %ctaid.x;
	setp.ne.s32 	%p15, %r2, 0;
	// begin inline asm
	fence.mbarrier_init.release.cluster;
	// end inline asm
	bar.sync 	0;
	add.s32 	%r98, %r69, -1;
	shr.s32 	%r99, %r98, 31;
	shr.u32 	%r100, %r99, 26;
	add.s32 	%r101, %r98, %r100;
	shr.s32 	%r102, %r101, 6;
	add.s32 	%r103, %r102, 1;
	neg.s32 	%r104, %r69;
	shr.s32 	%r105, %r104, 31;
	shr.u32 	%r106, %r105, 26;
	sub.s32 	%r107, %r106, %r69;
	shr.s32 	%r108, %r107, 6;
	neg.s32 	%r109, %r108;
	setp.gt.s32 	%p16, %r69, 0;
	selp.b32 	%r5, %r103, %r109, %p16;
	shl.b32 	%r6, %r97, 7;
	shl.b32 	%r7, %r3, 8;
	cvt.u64.u32 	%rd3, %r97;
	bar.sync 1, 128;
	ld.shared.u32 	%r8, [__dynamic_shmem__0+80];
	shl.b32 	%r9, %r1, 16;
	and.b32  	%r10, %r1, 31;
	and.b32  	%r11, %r1, 96;
	@%p15 bra 	$L__BB0_37;
	add.s32 	%r110, %r79, 72;
	// begin inline asm
	{
	.reg .pred       P1; 
	LAB_WAIT: 
	mbarrier.try_wait.parity.shared.b64 P1, [%r110], 1, 10000000; 
	@P1 bra.uni DONE; 
	bra.uni     LAB_WAIT; 
	DONE: 
	}
	// end inline asm
	min.s32 	%r261, %r5, 2;
	setp.lt.s32 	%p17, %r5, 1;
	mov.b32 	%r259, 1;
	mov.b32 	%r266, 0;
	mov.u32 	%r270, %r266;
	@%p17 bra 	$L__BB0_14;
	neg.s32 	%r258, %r261;
	mov.b32 	%r259, 1;
	mov.b32 	%r257, 0;
	cvta.param.u64 	%rd13, %rd9;
	cvta.param.u64 	%rd15, %rd10;
	mov.u32 	%r260, %r257;
$L__BB0_7:
	shl.b32 	%r118, %r260, 3;
	add.s32 	%r120, %r79, %r118;
	add.s32 	%r28, %r120, 32;
	// begin inline asm
	{
	.reg .pred       P1; 
	LAB_WAIT: 
	mbarrier.try_wait.parity.shared.b64 P1, [%r28], %r259, 10000000; 
	@P1 bra.uni DONE; 
	bra.uni     LAB_WAIT; 
	DONE: 
	}
	// end inline asm
	elect.sync 	%r121|%p18, -1;
	not.pred 	%p19, %p18;
	@%p19 bra 	$L__BB0_9;
	add.s32 	%r122, %r28, -32;
	mov.b32 	%r123, 49152;
	mbarrier.arrive.expect_tx.release.cta.shared::cta.b64 _, [%r122], %r123;
$L__BB0_9:
	add.s32 	%r124, %r260, 1;
	setp.eq.s32 	%p20, %r124, 4;
	selp.b32 	%r266, 0, %r124, %p20;
	selp.u32 	%r125, 1, 0, %p20;
	xor.b32  	%r259, %r259, %r125;
	shl.b32 	%r126, %r260, 14;
	add.s32 	%r31, %r4, %r126;
	add.s32 	%r32, %r28, -32;
	elect.sync 	%r127|%p21, -1;
	not.pred 	%p22, %p21;
	@%p22 bra 	$L__BB0_11;
	mov.u64 	%rd14, 0;
	cp.async.bulk.tensor.2d.shared::cluster.global.tile.mbarrier::complete_tx::bytes.L2::cache_hint [%r31], [%rd13, {%r257, %r6}], [%r32], %rd14;
$L__BB0_11:
	elect.sync 	%r128|%p23, -1;
	not.pred 	%p24, %p23;
	@%p24 bra 	$L__BB0_13;
	shl.b32 	%r129, %r260, 15;
	add.s32 	%r130, %r4, %r129;
	add.s32 	%r131, %r130, 65536;
	mov.u64 	%rd16, 0;
	cp.async.bulk.tensor.2d.shared::cluster.global.tile.mbarrier::complete_tx::bytes.L2::cache_hint [%r131], [%rd15, {%r257, %r7}], [%r32], %rd16;
$L__BB0_13:
	add.s32 	%r258, %r258, 1;
	setp.eq.s32 	%p25, %r258, 0;
	add.s32 	%r257, %r257, 64;
	mov.u32 	%r270, %r261;
	mov.u32 	%r260, %r266;
	@%p25 bra 	$L__BB0_14;
	bra.uni 	$L__BB0_7;
$L__BB0_14:
	setp.lt.s32 	%p26, %r5, 1;
	@%p26 bra 	$L__BB0_34;
	shr.u32 	%r135, %r80, 4;
	and.b32  	%r136, %r135, 16328;
	cvt.u64.u32 	%rd17, %r136;
	or.b64  	%rd18, %rd17, 4611756662049538048;
	mov.b64 	{%r19, %r18}, %rd18;
	add.s32 	%r137, %r4, 65536;
	shr.u32 	%r138, %r137, 4;
	and.b32  	%r139, %r138, 16328;
	cvt.u64.u32 	%rd19, %r139;
	or.b64  	%rd20, %rd19, 4611756662049538048;
	mov.b64 	{%r22, %r21}, %rd20;
	neg.s32 	%r262, %r5;
	mov.b32 	%r263, 0;
	cvta.param.u64 	%rd21, %rd9;
	cvta.param.u64 	%rd23, %rd10;
	mov.u32 	%r264, %r263;
	mov.u32 	%r268, %r266;
$L__BB0_16:
	setp.ge.u32 	%p27, %r261, %r5;
	@%p27 bra 	$L__BB0_23;
	shl.b32 	%r142, %r266, 3;
	add.s32 	%r144, %r79, %r142;
	add.s32 	%r42, %r144, 32;
	// begin inline asm
	{
	.reg .pred       P1; 
	LAB_WAIT: 
	mbarrier.try_wait.parity.shared.b64 P1, [%r42], %r259, 10000000; 
	@P1 bra.uni DONE; 
	bra.uni     LAB_WAIT; 
	DONE: 
	}
	// end inline asm
	elect.sync 	%r145|%p28, -1;
	not.pred 	%p29, %p28;
	@%p29 bra 	$L__BB0_19;
	add.s32 	%r146, %r42, -32;
	mov.b32 	%r147, 49152;
	mbarrier.arrive.expect_tx.release.cta.shared::cta.b64 _, [%r146], %r147;
$L__BB0_19:
	shl.b32 	%r43, %r270, 6;
	shl.b32 	%r148, %r266, 14;
	add.s32 	%r44, %r4, %r148;
	add.s32 	%r45, %r42, -32;
	elect.sync 	%r149|%p30, -1;
	not.pred 	%p31, %p30;
	@%p31 bra 	$L__BB0_21;
	mov.u64 	%rd22, 0;
	cp.async.bulk.tensor.2d.shared::cluster.global.tile.mbarrier::complete_tx::bytes.L2::cache_hint [%r44], [%rd21, {%r43, %r6}], [%r45], %rd22;
$L__BB0_21:
	add.s32 	%r150, %r266, 1;
	add.s32 	%r270, %r270, 1;
	setp.eq.s32 	%p32, %r150, 4;
	selp.b32 	%r268, 0, %r150, %p32;
	selp.u32 	%r151, 1, 0, %p32;
	xor.b32  	%r259, %r259, %r151;
	elect.sync 	%r152|%p33, -1;
	not.pred 	%p34, %p33;
	@%p34 bra 	$L__BB0_23;
	shl.b32 	%r153, %r266, 15;
	add.s32 	%r154, %r4, %r153;
	add.s32 	%r155, %r154, 65536;
	mov.u64 	%rd24, 0;
	cp.async.bulk.tensor.2d.shared::cluster.global.tile.mbarrier::complete_tx::bytes.L2::cache_hint [%r155], [%rd23, {%r43, %r7}], [%r45], %rd24;
$L__BB0_23:
	shl.b32 	%r158, %r264, 3;
	add.s32 	%r156, %r79, %r158;
	// begin inline asm
	{
	.reg .pred       P1; 
	LAB_WAIT: 
	mbarrier.try_wait.parity.shared.b64 P1, [%r156], %r263, 10000000; 
	@P1 bra.uni DONE; 
	bra.uni     LAB_WAIT; 
	DONE: 
	}
	// end inline asm
	add.s32 	%r53, %r264, 1;
	shl.b32 	%r160, %r264, 10;
	add.s32 	%r54, %r160, %r19;
	shl.b32 	%r161, %r264, 11;
	add.s32 	%r55, %r161, %r22;
	selp.b32 	%r162, 16384, 0, %p12;
	selp.b32 	%r163, 138420240, 138412048, %p11;
	or.b32  	%r56, %r163, %r162;
	elect.sync 	%r164|%p35, -1;
	not.pred 	%p36, %p35;
	@%p36 bra 	$L__BB0_25;
	mov.b64 	%rd25, {%r55, %r21};
	mov.b64 	%rd26, {%r54, %r18};
	mov.b32 	%r165, 0;
	tcgen05.mma.cta_group::1.kind::f16 [%r8], %rd26, %rd25, %r56, {%r165, %r165, %r165, %r165}, %p54;
$L__BB0_25:
	elect.sync 	%r166|%p37, -1;
	not.pred 	%p38, %p37;
	@%p38 bra 	$L__BB0_27;
	add.s32 	%r167, %r55, 2;
	mov.b64 	%rd27, {%r167, %r21};
	add.s32 	%r168, %r54, 2;
	mov.b64 	%rd28, {%r168, %r18};
	mov.pred 	%p39, -1;
	mov.b32 	%r169, 0;
	tcgen05.mma.cta_group::1.kind::f16 [%r8], %rd28, %rd27, %r56, {%r169, %r169, %r169, %r169}, %p39;
$L__BB0_27:
	elect.sync 	%r170|%p40, -1;
	not.pred 	%p41, %p40;
	@%p41 bra 	$L__BB0_29;
	add.s32 	%r171, %r55, 4;
	mov.b64 	%rd29, {%r171, %r21};
	add.s32 	%r172, %r54, 4;
	mov.b64 	%rd30, {%r172, %r18};
	mov.pred 	%p42, -1;
	mov.b32 	%r173, 0;
	tcgen05.mma.cta_group::1.kind::f16 [%r8], %rd30, %rd29, %r56, {%r173, %r173, %r173, %r173}, %p42;
$L__BB0_29:
	add.s32 	%r57, %r54, 6;
	add.s32 	%r58, %r55, 6;
	elect.sync 	%r174|%p43, -1;
	not.pred 	%p44, %p43;
	@%p44 bra 	$L__BB0_31;
	mov.b64 	%rd31, {%r58, %r21};
	mov.b64 	%rd32, {%r57, %r18};
	mov.pred 	%p45, -1;
	mov.b32 	%r175, 0;
	tcgen05.mma.cta_group::1.kind::f16 [%r8], %rd32, %rd31, %r56, {%r175, %r175, %r175, %r175}, %p45;
$L__BB0_31:
	elect.sync 	%r176|%p46, -1;
	not.pred 	%p47, %p46;
	@%p47 bra 	$L__BB0_33;
	add.s32 	%r177, %r156, 32;
	tcgen05.commit.cta_group::1.mbarrier::arrive::one.shared::cluster.b64 [%r177];
$L__BB0_33:
	setp.eq.s32 	%p48, %r53, 4;
	add.s32 	%r262, %r262, 1;
	setp.ne.s32 	%p49, %r262, 0;
	add.s32 	%r261, %r261, 1;
	selp.u32 	%r178, 1, 0, %p48;
	xor.b32  	%r263, %r263, %r178;
	selp.b32 	%r264, 0, %r53, %p48;
	mov.pred 	%p54, -1;
	mov.u32 	%r266, %r268;
	@%p49 bra 	$L__BB0_16;
$L__BB0_34:
	elect.sync 	%r179|%p50, -1;
	not.pred 	%p51, %p50;
	@%p51 bra 	$L__BB0_36;
	add.s32 	%r181, %r79, 64;
	tcgen05.commit.cta_group::1.mbarrier::arrive::one.shared::cluster.b64 [%r181];
$L__BB0_36:
	tcgen05.relinquish_alloc_permit.cta_group::1.sync.aligned;
$L__BB0_37:
	add.s32 	%r182, %r79, 64;
	// begin inline asm
	{
	.reg .pred       P1; 
	LAB_WAIT: 
	mbarrier.try_wait.parity.shared.b64 P1, [%r182], 0, 10000000; 
	@P1 bra.uni DONE; 
	bra.uni     LAB_WAIT; 
	DONE: 
	}
	// end inline asm
	cvt.u64.u32 	%rd33, %r10;
	shl.b64 	%rd34, %rd3, 7;
	or.b64  	%rd35, %rd34, %rd33;
	cvt.u64.u32 	%rd36, %r11;
	add.s64 	%rd37, %rd35, %rd36;
	and.b32  	%r185, %r9, 6291456;
	add.s32 	%r271, %r8, %r185;
	mul.lo.s64 	%rd38, %rd8, %rd37;
	add.s64 	%rd39, %rd38, %rd38;
	mul.wide.u32 	%rd40, %r3, 512;
	add.s64 	%rd41, %rd39, %rd40;
	add.s64 	%rd42, %rd41, %rd7;
	add.s64 	%rd43, %rd42, 64;
	mov.b32 	%r272, 0;
$L__BB0_38:
	tcgen05.ld.sync.aligned.32x32b.x64.b32 {%r186, %r187, %r188, %r189, %r190, %r191, %r192, %r193, %r194, %r195, %r196, %r197, %r198, %r199, %r200, %r201, %r202, %r203, %r204, %r205, %r206, %r207, %r208, %r209, %r210, %r211, %r212, %r213, %r214, %r215, %r216, %r217, %r218, %r219, %r220, %r221, %r222, %r223, %r224, %r225, %r226, %r227, %r228, %r229, %r230, %r231, %r232, %r233, %r234, %r235, %r236, %r237, %r238, %r239, %r240, %r241, %r242, %r243, %r244, %r245, %r246, %r247, %r248, %r249}, [%r271];
	mov.b32 	%f1, %r186;
	mov.b32 	%f2, %r187;
	mov.b32 	%f3, %r188;
	mov.b32 	%f4, %r189;
	mov.b32 	%f5, %r190;
	mov.b32 	%f6, %r191;
	mov.b32 	%f7, %r192;
	mov.b32 	%f8, %r193;
	mov.b32 	%f9, %r194;
	mov.b32 	%f10, %r195;
	mov.b32 	%f11, %r196;
	mov.b32 	%f12, %r197;
	mov.b32 	%f13, %r198;
	mov.b32 	%f14, %r199;
	mov.b32 	%f15, %r200;
	mov.b32 	%f16, %r201;
	mov.b32 	%f17, %r202;
	mov.b32 	%f18, %r203;
	mov.b32 	%f19, %r204;
	mov.b32 	%f20, %r205;
	mov.b32 	%f21, %r206;
	mov.b32 	%f22, %r207;
	mov.b32 	%f23, %r208;
	mov.b32 	%f24, %r209;
	mov.b32 	%f25, %r210;
	mov.b32 	%f26, %r211;
	mov.b32 	%f27, %r212;
	mov.b32 	%f28, %r213;
	mov.b32 	%f29, %r214;
	mov.b32 	%f30, %r215;
	mov.b32 	%f31, %r216;
	mov.b32 	%f32, %r217;
	mov.b32 	%f33, %r218;
	mov.b32 	%f34, %r219;
	mov.b32 	%f35, %r220;
	mov.b32 	%f36, %r221;
	mov.b32 	%f37, %r222;
	mov.b32 	%f38, %r223;
	mov.b32 	%f39, %r224;
	mov.b32 	%f40, %r225;
	mov.b32 	%f41, %r226;
	mov.b32 	%f42, %r227;
	mov.b32 	%f43, %r228;
	mov.b32 	%f44, %r229;
	mov.b32 	%f45, %r230;
	mov.b32 	%f46, %r231;
	mov.b32 	%f47, %r232;
	mov.b32 	%f48, %r233;
	mov.b32 	%f49, %r234;
	mov.b32 	%f50, %r235;
	mov.b32 	%f51, %r236;
	mov.b32 	%f52, %r237;
	mov.b32 	%f53, %r238;
	mov.b32 	%f54, %r239;
	mov.b32 	%f55, %r240;
	mov.b32 	%f56, %r241;
	mov.b32 	%f57, %r242;
	mov.b32 	%f58, %r243;
	mov.b32 	%f59, %r244;
	mov.b32 	%f60, %r245;
	mov.b32 	%f61, %r246;
	mov.b32 	%f62, %r247;
	mov.b32 	%f63, %r248;
	mov.b32 	%f64, %r249;
	cvt.rn.f16.f32 	%rs7, %f64;
	cvt.rn.f16.f32 	%rs8, %f63;
	cvt.rn.f16.f32 	%rs9, %f62;
	cvt.rn.f16.f32 	%rs10, %f61;
	cvt.rn.f16.f32 	%rs11, %f60;
	cvt.rn.f16.f32 	%rs12, %f59;
	cvt.rn.f16.f32 	%rs13, %f58;
	cvt.rn.f16.f32 	%rs14, %f57;
	cvt.rn.f16.f32 	%rs15, %f56;
	cvt.rn.f16.f32 	%rs16, %f55;
	cvt.rn.f16.f32 	%rs17, %f54;
	cvt.rn.f16.f32 	%rs18, %f53;
	cvt.rn.f16.f32 	%rs19, %f52;
	cvt.rn.f16.f32 	%rs20, %f51;
	cvt.rn.f16.f32 	%rs21, %f50;
	cvt.rn.f16.f32 	%rs22, %f49;
	cvt.rn.f16.f32 	%rs23, %f48;
	cvt.rn.f16.f32 	%rs24, %f47;
	cvt.rn.f16.f32 	%rs25, %f46;
	cvt.rn.f16.f32 	%rs26, %f45;
	cvt.rn.f16.f32 	%rs27, %f44;
	cvt.rn.f16.f32 	%rs28, %f43;
	cvt.rn.f16.f32 	%rs29, %f42;
	cvt.rn.f16.f32 	%rs30, %f41;
	cvt.rn.f16.f32 	%rs31, %f40;
	cvt.rn.f16.f32 	%rs32, %f39;
	cvt.rn.f16.f32 	%rs33, %f38;
	cvt.rn.f16.f32 	%rs34, %f37;
	cvt.rn.f16.f32 	%rs35, %f36;
	cvt.rn.f16.f32 	%rs36, %f35;
	cvt.rn.f16.f32 	%rs37, %f34;
	cvt.rn.f16.f32 	%rs38, %f33;
	cvt.rn.f16.f32 	%rs39, %f32;
	cvt.rn.f16.f32 	%rs40, %f31;
	cvt.rn.f16.f32 	%rs41, %f30;
	cvt.rn.f16.f32 	%rs42, %f29;
	cvt.rn.f16.f32 	%rs43, %f28;
	cvt.rn.f16.f32 	%rs44, %f27;
	cvt.rn.f16.f32 	%rs45, %f26;
	cvt.rn.f16.f32 	%rs46, %f25;
	cvt.rn.f16.f32 	%rs47, %f24;
	cvt.rn.f16.f32 	%rs48, %f23;
	cvt.rn.f16.f32 	%rs49, %f22;
	cvt.rn.f16.f32 	%rs50, %f21;
	cvt.rn.f16.f32 	%rs51, %f20;
	cvt.rn.f16.f32 	%rs52, %f19;
	cvt.rn.f16.f32 	%rs53, %f18;
	cvt.rn.f16.f32 	%rs54, %f17;
	cvt.rn.f16.f32 	%rs55, %f16;
	cvt.rn.f16.f32 	%rs56, %f15;
	cvt.rn.f16.f32 	%rs57, %f14;
	cvt.rn.f16.f32 	%rs58, %f13;
	cvt.rn.f16.f32 	%rs59, %f12;
	cvt.rn.f16.f32 	%rs60, %f11;
	cvt.rn.f16.f32 	%rs61, %f10;
	cvt.rn.f16.f32 	%rs62, %f9;
	cvt.rn.f16.f32 	%rs63, %f8;
	cvt.rn.f16.f32 	%rs64, %f7;
	cvt.rn.f16.f32 	%rs65, %f6;
	cvt.rn.f16.f32 	%rs66, %f5;
	cvt.rn.f16.f32 	%rs67, %f4;
	cvt.rn.f16.f32 	%rs68, %f3;
	cvt.rn.f16.f32 	%rs69, %f2;
	cvt.rn.f16.f32 	%rs70, %f1;
	st.global.b16 	[%rd43+-64], %rs70;
	st.global.b16 	[%rd43+-62], %rs69;
	st.global.b16 	[%rd43+-60], %rs68;
	st.global.b16 	[%rd43+-58], %rs67;
	st.global.b16 	[%rd43+-56], %rs66;
	st.global.b16 	[%rd43+-54], %rs65;
	st.global.b16 	[%rd43+-52], %rs64;
	st.global.b16 	[%rd43+-50], %rs63;
	st.global.b16 	[%rd43+-48], %rs62;
	st.global.b16 	[%rd43+-46], %rs61;
	st.global.b16 	[%rd43+-44], %rs60;
	st.global.b16 	[%rd43+-42], %rs59;
	st.global.b16 	[%rd43+-40], %rs58;
	st.global.b16 	[%rd43+-38], %rs57;
	st.global.b16 	[%rd43+-36], %rs56;
	st.global.b16 	[%rd43+-34], %rs55;
	st.global.b16 	[%rd43+-32], %rs54;
	st.global.b16 	[%rd43+-30], %rs53;
	st.global.b16 	[%rd43+-28], %rs52;
	st.global.b16 	[%rd43+-26], %rs51;
	st.global.b16 	[%rd43+-24], %rs50;
	st.global.b16 	[%rd43+-22], %rs49;
	st.global.b16 	[%rd43+-20], %rs48;
	st.global.b16 	[%rd43+-18], %rs47;
	st.global.b16 	[%rd43+-16], %rs46;
	st.global.b16 	[%rd43+-14], %rs45;
	st.global.b16 	[%rd43+-12], %rs44;
	st.global.b16 	[%rd43+-10], %rs43;
	st.global.b16 	[%rd43+-8], %rs42;
	st.global.b16 	[%rd43+-6], %rs41;
	st.global.b16 	[%rd43+-4], %rs40;
	st.global.b16 	[%rd43+-2], %rs39;
	st.global.b16 	[%rd43], %rs38;
	st.global.b16 	[%rd43+2], %rs37;
	st.global.b16 	[%rd43+4], %rs36;
	st.global.b16 	[%rd43+6], %rs35;
	st.global.b16 	[%rd43+8], %rs34;
	st.global.b16 	[%rd43+10], %rs33;
	st.global.b16 	[%rd43+12], %rs32;
	st.global.b16 	[%rd43+14], %rs31;
	st.global.b16 	[%rd43+16], %rs30;
	st.global.b16 	[%rd43+18], %rs29;
	st.global.b16 	[%rd43+20], %rs28;
	st.global.b16 	[%rd43+22], %rs27;
	st.global.b16 	[%rd43+24], %rs26;
	st.global.b16 	[%rd43+26], %rs25;
	st.global.b16 	[%rd43+28], %rs24;
	st.global.b16 	[%rd43+30], %rs23;
	st.global.b16 	[%rd43+32], %rs22;
	st.global.b16 	[%rd43+34], %rs21;
	st.global.b16 	[%rd43+36], %rs20;
	st.global.b16 	[%rd43+38], %rs19;
	st.global.b16 	[%rd43+40], %rs18;
	st.global.b16 	[%rd43+42], %rs17;
	st.global.b16 	[%rd43+44], %rs16;
	st.global.b16 	[%rd43+46], %rs15;
	st.global.b16 	[%rd43+48], %rs14;
	st.global.b16 	[%rd43+50], %rs13;
	st.global.b16 	[%rd43+52], %rs12;
	st.global.b16 	[%rd43+54], %rs11;
	st.global.b16 	[%rd43+56], %rs10;
	st.global.b16 	[%rd43+58], %rs9;
	st.global.b16 	[%rd43+60], %rs8;
	st.global.b16 	[%rd43+62], %rs7;
	add.s32 	%r272, %r272, 1;
	add.s32 	%r271, %r271, 64;
	add.s64 	%rd43, %rd43, 128;
	setp.ne.s32 	%p52, %r272, 4;
	@%p52 bra 	$L__BB0_38;
	setp.ne.s32 	%p53, %r2, 0;
	mov.u32 	%r250, __dynamic_shmem__0;
	add.s32 	%r251, %r250, 72;
	mov.b32 	%r252, 1;
	mbarrier.arrive.release.cta.shared::cta.b64 _, [%r251], %r252;
	bar.sync 	%r252;
	@%p53 bra 	$L__BB0_41;
	mov.b32 	%r253, 512;
	tcgen05.dealloc.cta_group::1.sync.aligned.b32 %r8, %r253;
$L__BB0_41:
	ret;

}
 