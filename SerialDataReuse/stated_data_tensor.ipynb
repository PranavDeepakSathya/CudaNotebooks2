{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "700853b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class MemoryState:\n",
    "    def __init__(self, name, unit_access_cost, capacity_bytes,level):\n",
    "        self.level = level\n",
    "        self.name = name\n",
    "        self.unit_access_cost = unit_access_cost\n",
    "        self.capacity_bytes = capacity_bytes\n",
    "\n",
    "    def __repr__(self):\n",
    "        # Format bytes to KB/MB for readability\n",
    "        if self.capacity_bytes >= 1024**3:\n",
    "            cap_str = f\"{self.capacity_bytes / 1024**3:.1f} GB\"\n",
    "        elif self.capacity_bytes >= 1024**2:\n",
    "            cap_str = f\"{self.capacity_bytes / 1024**2:.1f} MB\"\n",
    "        elif self.capacity_bytes >= 1024:\n",
    "            cap_str = f\"{self.capacity_bytes / 1024:.1f} KB\"\n",
    "        else:\n",
    "            cap_str = f\"{self.capacity_bytes} B\"\n",
    "            \n",
    "        return f\"{self.name}(Cost={self.unit_access_cost}, Max={cap_str})\"\n",
    "\n",
    "\n",
    "\n",
    "class AugmentedTensor:\n",
    "    def __init__(self, data, state):\n",
    "        \"\"\"\n",
    "        :param data: The actual numpy array (or shape, if you want empty)\n",
    "        :param state: MemoryState object (holds the cost logic)\n",
    "        \"\"\"\n",
    "        # 1. Initialize Data\n",
    "        if isinstance(data, (tuple, list)):\n",
    "            self.data = np.zeros(data) # Default float64, be careful!\n",
    "        else:\n",
    "            self.data = np.array(data)\n",
    "            \n",
    "        self.state = state\n",
    "        self.total_cost = 0.0\n",
    "\n",
    "        # 2. CAPACITY CHECK\n",
    "        tensor_bytes = self.data.nbytes\n",
    "        if tensor_bytes > self.state.capacity_bytes:\n",
    "            raise MemoryError(\n",
    "                f\"OOM: Cannot allocate tensor of size {tensor_bytes} bytes \"\n",
    "                f\"in {self.state.name} (Max: {self.state.capacity_bytes} bytes).\"\n",
    "            )\n",
    "\n",
    "    def load(self, source_tensor):\n",
    "        \"\"\"\n",
    "        B.load(A) -> Copies data from A to B.\n",
    "        The COST is incurred by A (the Source).\n",
    "        \"\"\"\n",
    "        # 1. Check if source is actually an AugmentedTensor (or a slice of one)\n",
    "        # Note: We do NOT need to check capacity here, because 'self' (B) \n",
    "        # was already checked during __init__.\n",
    "        \n",
    "        # 2. Incur Cost on the SOURCE\n",
    "        if isinstance(source_tensor, TensorSlice):\n",
    "            source_tensor.report_access()\n",
    "            # Copy data (Broadcasting/Slicing handled by numpy)\n",
    "            self.data[:] = source_tensor.data\n",
    "        elif isinstance(source_tensor, AugmentedTensor):\n",
    "            cost = source_tensor.data.size * source_tensor.state.unit_access_cost\n",
    "            source_tensor.total_cost += cost\n",
    "            self.data[:] = source_tensor.data\n",
    "        else:\n",
    "            raise ValueError(\"Source must be an AugmentedTensor or TensorSlice\")\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        return TensorSlice(self, key)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"[{self.state.name}] Shape:{self.data.shape} | Size:{self.data.nbytes} B | AccCost: {self.total_cost}\"\n",
    "\n",
    "\n",
    "class TensorSlice:\n",
    "    \"\"\"\n",
    "    Helper class to handle A[slice].\n",
    "    When this is passed to load(), we charge the PARENT.\n",
    "    \"\"\"\n",
    "    def __init__(self, parent, slice_key):\n",
    "        self.parent = parent\n",
    "        self.data = parent.data[slice_key] # The actual data view\n",
    "        self.state = parent.state # Same state as parent\n",
    "\n",
    "    def report_access(self):\n",
    "        \"\"\" Charge the parent for this slice's size \"\"\"\n",
    "        cost = self.data.size * self.state.unit_access_cost\n",
    "        self.parent.total_cost += cost\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "340daaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "H100 = {\"N_sms\":132,\"rmem_kb\":228, \"SMEM_kb\":228, \"DSMEM_mb\":3.5, \"GMEM_gb\":80}\n",
    "\n",
    "GMEM = MemoryState(\"gmem\",500, H100[\"GMEM_gb\"]*1024**3,3)\n",
    "DSMEM = MemoryState(\"DSMEM\",100, H100[\"DSMEM_mb\"]*1024**2,2)\n",
    "SMEM = MemoryState(\"SMEM\", 30, H100[\"SMEM_kb\"]*1024,1)\n",
    "RMEM = MemoryState(\"RMEM\", 1, H100[\"rmem_kb\"]*1024,0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711ed0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will make the symplifying assumption of tiling-shift isomorphism across paralell processors, that is, \n",
    "#we only care about what cluster_0 processes (its whole loop), and every part block_0 in cluster_0 processes, and so on, \n",
    "#and also outer K is literally always the best.\n",
    "\n",
    "class make_matmul:\n",
    "  def __init__(self,g_shape :Tuple[int,int,int],c_shape:Tuple[int,int], c_loop_shape:Tuple[int,int,int],\n",
    "               b_shape:Tuple[int,int],b_loop_shape:Tuple[int,int,int], t_shape, t_loop_shape:Tuple[int,int,int]):\n",
    "    \n",
    "    self.M,self.K,self.N = g_shape \n",
    "    self.CM, self.CN = c_shape \n",
    "    self.cm,self.ck,self.cn = c_loop_shape \n",
    "    self.BM, self.BN = b_shape \n",
    "    self.bm, self.bk, self.bn = b_loop_shape \n",
    "    self.TM, self.TN = t_shape\n",
    "    self.tm, self.tk, self.tn = t_loop_shape\n",
    "    \n",
    "    self.gA = AugmentedTensor(np.random.randn(self.M,self.K), GMEM)\n",
    "    self.gB = AugmentedTensor(np.random.randn(self.K,self.N), GMEM)\n",
    "    self.gC = AugmentedTensor(np.zeros((self.M, self.N)), GMEM)\n",
    "    \n",
    "    self.cA = AugmentedTensor(np.zeros((self.cm,self.ck)), DSMEM)\n",
    "    self.cB = AugmentedTensor(np.zeros((self.ck,self.cn)), DSMEM)\n",
    "    self.cC = AugmentedTensor(np.zeros((self.cm,self.cn)), DSMEM)\n",
    "\n",
    "    self.sA = AugmentedTensor(np.zeros((self.bm, self.bk)), SMEM)\n",
    "    self.sB = AugmentedTensor(np.zeros((self.bk, self.bn)), SMEM)\n",
    "    self.sC = AugmentedTensor(np.zeros((self.bm, self.bn)), SMEM)\n",
    "    \n",
    "    self.rA = AugmentedTensor(np.zeros((self.tm,self.tk)), RMEM)\n",
    "    self.rB = AugmentedTensor(np.zeros((self.tk,self.tn)), RMEM)\n",
    "    self.rC = AugmentedTensor(np.zeros((self.tm, self.tn)),RMEM)\n",
    "    \n",
    "    \n",
    "def run(self): \n",
    "  for ck_idx in range(0, self.K, self.ck): \n",
    "    for cm_idx in range(0,self.CM, self.cm): \n",
    "      for cn_idx in range(0,self.CN, self.cn): \n",
    "        self.cA.load(self.gA[cm_idx:cm_idx+self.cm,ck_idx:ck_idx+self.ck])\n",
    "        self.cB.load(self.gB[ck_idx:ck_idx+self.ck,cn_idx:cn_idx+self.cn])\n",
    "        for bk_idx in range(0, self.ck, self.bk):\n",
    "          for bm_idx in range(0,self.BM, self.bm): \n",
    "            for bn_idx in range(0, self.BN, self.bn):\n",
    "              self.sA.load(self.cA[bm_idx:bm_idx+self.bm,bk_idx:bk_idx+self.bk])\n",
    "              self.sB.load(self.cB[bk_idx:bk_idx+self.bk,bn_idx:bn_idx+self.bn])\n",
    "              for tk_idx in range(0, self.bk, self.tk): \n",
    "                for tm_idx in range(0,self.TM, self.tm): \n",
    "                  for tn_idx in range(0, self.TN, self.tn): \n",
    "                    self.rA.load(self.sA[tm_idx:tm_idx+self.tm,tk_idx:tk_idx+self.tk])\n",
    "                    self.rB.load(self.sB[tk_idx:tk_idx+self.tk,tn_idx:tn_idx+self.tn])\n",
    "                    for k in range(0, tk_idx): \n",
    "                      for m in range(0, tm_idx):\n",
    "                        for n in range(0, tn_idx): \n",
    "                      \n",
    "                            \n",
    "                          c_global_m_coord = tm_idx + bm_idx + cm_idx + m\n",
    "                          c_global_n_coord = tn_idx + bn_idx + cn_idx + n \n",
    "                          self.rC.load(self.gC[c_global_m_coord, c_global_n_coord])                          \n",
    "              \n",
    "        \n",
    "        \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449983b1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
