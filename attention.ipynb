{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6da5616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b8969a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention: \n",
    "  def __init__ (self, batch_size:int, sequence_length:int, model_dimension, n_heads):\n",
    "    assert model_dimension % n_heads == 0 \n",
    "    self.B = batch_size\n",
    "    self.L = sequence_length\n",
    "    self.D = model_dimension\n",
    "    self.H = n_heads\n",
    "    self.dk = self.D // self.H \n",
    "    \n",
    "  def forward(self, X, W_Q, W_K, W_V): \n",
    "    # 1. Fuse weights (D, 3D)\n",
    "    W_Q_K_V = torch.cat([W_Q, W_K, W_V], dim=1)\n",
    "    \n",
    "    # 2. Project Input: (B, L, D) @ (D, 3D) -> (B, L, 3D)\n",
    "    projs = torch.matmul(X, W_Q_K_V)\n",
    "    \n",
    "    # 3. Reshape to isolate 3 (Q,K,V) and Heads\n",
    "    # Shape becomes: (B, L, 3, H, Dk)\n",
    "    T = projs.reshape(self.B, self.L, 3, self.H, self.dk)\n",
    "    \n",
    "    # 4. Permute to get Heads *before* Sequence Length\n",
    "    # We want: (3, B, H, L, Dk)\n",
    "    # 2 -> 0 (The QKV dimension moves to front)\n",
    "    # 0 -> 1 (Batch stays)\n",
    "    # 3 -> 2 (Heads move before Length)\n",
    "    # 1 -> 3 (Length moves after Heads)\n",
    "    # 4 -> 4 (Head Dim stays last)\n",
    "    T = T.permute(2, 0, 3, 1, 4)\n",
    "    # 5. Split into Q, K, V\n",
    "    Q, KT, V = T[0], T[1].permute(0,1,3,2), T[2]\n",
    "    all_head_out = torch.matmul(torch.softmax((torch.rsqrt(torch.tensor(self.dk)))*(torch.matmul(Q, KT)), dim =3), V)\n",
    "    return (all_head_out).permute(0,2,1,3).reshape(self.B, self.L, self.D)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e1229fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16 \n",
    "sequence_length = 4\n",
    "model_dimension = 256\n",
    "n_heads = 8\n",
    "\n",
    "X = torch.randn(batch_size, sequence_length, model_dimension)\n",
    "W_Q, W_K, W_V = torch.randn(model_dimension, model_dimension), torch.randn(model_dimension, model_dimension), torch.randn(model_dimension, model_dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "578407e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "H = MultiHeadAttention(batch_size, sequence_length, model_dimension, n_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a0819fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = H.forward(X,W_Q, W_K, W_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9b34e1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "print(Q.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63923071",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 4, 256])\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df68e5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
