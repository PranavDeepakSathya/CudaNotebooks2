{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df1ed092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source files will be saved in \"/tmp/tmp5pdr6r__\".\n"
     ]
    }
   ],
   "source": [
    "%load_ext nvcc4jupyter\n",
    "\n",
    "from nvcc4jupyter import set_defaults\n",
    "set_defaults(compiler_args='-arch=sm_100a -Xptxas=-v -O0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "556ee94a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 1.613792 ms\n",
      "Performance: 0.005076 GFLOPS\n",
      "Check C[0]: 16.000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "#include <mma.h>\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h> \n",
    "#include <cuda.h> \n",
    "#include <cuda_fp16.h>\n",
    "\n",
    "constexpr int N_warps = 1; \n",
    "constexpr int atom_M = 16; \n",
    "constexpr int atom_N = 16; \n",
    "constexpr int atom_K = 16;\n",
    "\n",
    "size_t tensor_size = atom_M * atom_K * sizeof(half);\n",
    "size_t C_size = atom_M * atom_N * sizeof(float);\n",
    "\n",
    "using namespace nvcuda;\n",
    "\n",
    "__global__ void wmma_ker(half *a, half *b, float *c) {\n",
    "   wmma::fragment<wmma::matrix_a, 16, 16, 16, half, wmma::col_major> a_frag;\n",
    "   wmma::fragment<wmma::matrix_b, 16, 16, 16, half, wmma::row_major> b_frag;\n",
    "   wmma::fragment<wmma::accumulator, 16, 16, 16, float> c_frag;\n",
    "\n",
    "   wmma::fill_fragment(c_frag, 0.0f);\n",
    "\n",
    "   wmma::load_matrix_sync(a_frag, a, 16);\n",
    "   wmma::load_matrix_sync(b_frag, b, 16);\n",
    "\n",
    "   wmma::mma_sync(c_frag, a_frag, b_frag, c_frag);\n",
    "\n",
    "   wmma::store_matrix_sync(c, c_frag, 16, wmma::mem_row_major);\n",
    "}\n",
    "\n",
    "int main() {\n",
    "  float *C, *C_d;\n",
    "  half *A, *A_d, *B, *B_d;\n",
    "\n",
    "  cudaHostAlloc(&A, tensor_size, cudaHostAllocDefault); \n",
    "  cudaHostAlloc(&B, tensor_size, cudaHostAllocDefault); \n",
    "  cudaHostAlloc(&C, C_size, cudaHostAllocDefault); \n",
    "  \n",
    "  cudaMalloc(&A_d, tensor_size); \n",
    "  cudaMalloc(&B_d, tensor_size); \n",
    "  cudaMalloc(&C_d, C_size);\n",
    "\n",
    "  for (int i = 0; i < atom_M * atom_K; i++) {\n",
    "      A[i] = (half)1.0f;\n",
    "      B[i] = (half)1.0f;\n",
    "  }\n",
    "\n",
    "  cudaMemcpy(A_d, A, tensor_size, cudaMemcpyHostToDevice);\n",
    "  cudaMemcpy(B_d, B, tensor_size, cudaMemcpyHostToDevice);\n",
    "  cudaMemset(C_d, 0, C_size);\n",
    "\n",
    "  cudaEvent_t start, stop;\n",
    "  cudaEventCreate(&start);\n",
    "  cudaEventCreate(&stop);\n",
    "\n",
    "  cudaEventRecord(start);\n",
    "  wmma_ker<<<1, 32>>>(A_d, B_d, C_d);\n",
    "  cudaEventRecord(stop);\n",
    "\n",
    "  cudaEventSynchronize(stop);\n",
    "  \n",
    "  float milliseconds = 0;\n",
    "  cudaEventElapsedTime(&milliseconds, start, stop);\n",
    "\n",
    "  cudaMemcpy(C, C_d, C_size, cudaMemcpyDeviceToHost);\n",
    "\n",
    "  double ops = 2.0 * atom_M * atom_N * atom_K; \n",
    "  double gflops = (ops / 1e9) / (milliseconds / 1000.0f);\n",
    "\n",
    "  printf(\"Time: %f ms\\n\", milliseconds);\n",
    "  printf(\"Performance: %f GFLOPS\\n\", gflops);\n",
    "  printf(\"Check C[0]: %f\\n\", C[0]);\n",
    "\n",
    "  cudaFreeHost(A); cudaFreeHost(B); cudaFreeHost(C);\n",
    "  cudaFree(A_d); cudaFree(B_d); cudaFree(C_d);\n",
    "  \n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a19e35cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 0.212352 ms\n",
      "Performance: 0.038577 GFLOPS\n",
      "Check C[0]: 16.000000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "#include <mma.h>\n",
    "#include <stdio.h>\n",
    "#include <cuda_runtime.h> \n",
    "#include <cuda.h> \n",
    "#include <cuda_fp16.h>\n",
    "\n",
    "constexpr int N_warps = 1; \n",
    "constexpr int atom_M = 16; \n",
    "constexpr int atom_N = 16; \n",
    "constexpr int atom_K = 16;\n",
    "\n",
    "size_t tensor_size = atom_M * atom_K * sizeof(half);\n",
    "size_t C_size = atom_M * atom_N * sizeof(float);\n",
    "\n",
    "using namespace nvcuda;\n",
    "\n",
    "__global__ void wmma_ker(half *a, half *b, float *c) {\n",
    "  wmma::fragment<wmma::matrix_a, 16, 16, 16, half, wmma::col_major> a_frag;\n",
    "  wmma::fragment<wmma::matrix_b, 16, 16, 16, half, wmma::row_major> b_frag;\n",
    "  wmma::fragment<wmma::accumulator, 16, 16, 16, float> c_frag;\n",
    "  \n",
    "  __shared__ alignas(256) half As[32*32];\n",
    "  __shared__ alignas(256) half Bs[32*32];\n",
    "  int t = threadIdx.x; \n",
    "  int t_col = t % 16; \n",
    "  int t_row = t / 16;\n",
    "  \n",
    "  for (int i = 0; i < 16; i+= 2)\n",
    "  {\n",
    "    int idx = (t_row + i)*16 + t_col; \n",
    "    As[(3*16*16) + idx] = a[idx];\n",
    "    Bs[(3*16*16) + idx] = b[idx];\n",
    "    \n",
    "  }\n",
    "\n",
    "  __syncthreads();\n",
    "\n",
    "  wmma::fill_fragment(c_frag, 0.0f);\n",
    "\n",
    "  wmma::load_matrix_sync(a_frag, &As[(3*16*16)], 16);\n",
    "  wmma::load_matrix_sync(b_frag, &Bs[(3*16*16)], 16);\n",
    "\n",
    "  wmma::mma_sync(c_frag, a_frag, b_frag, c_frag);\n",
    "\n",
    "  wmma::store_matrix_sync(c, c_frag, 16, wmma::mem_row_major);\n",
    "  \n",
    "}\n",
    "\n",
    "int main() {\n",
    "  float *C, *C_d;\n",
    "  half *A, *A_d, *B, *B_d;\n",
    "\n",
    "  cudaHostAlloc(&A, tensor_size, cudaHostAllocDefault); \n",
    "  cudaHostAlloc(&B, tensor_size, cudaHostAllocDefault); \n",
    "  cudaHostAlloc(&C, C_size, cudaHostAllocDefault); \n",
    "\n",
    "  cudaMalloc(&A_d, tensor_size); \n",
    "  cudaMalloc(&B_d, tensor_size); \n",
    "  cudaMalloc(&C_d, C_size);\n",
    "\n",
    "  for (int i = 0; i < atom_M * atom_K; i++) {\n",
    "      A[i] = (half)1.0f;\n",
    "      B[i] = (half)1.0f;\n",
    "  }\n",
    "\n",
    "  cudaMemcpy(A_d, A, tensor_size, cudaMemcpyHostToDevice);\n",
    "  cudaMemcpy(B_d, B, tensor_size, cudaMemcpyHostToDevice);\n",
    "  cudaMemset(C_d, 0, C_size);\n",
    "\n",
    "  cudaEvent_t start, stop;\n",
    "  cudaEventCreate(&start);\n",
    "  cudaEventCreate(&stop);\n",
    "\n",
    "  cudaEventRecord(start);\n",
    "  wmma_ker<<<1, 32>>>(A_d, B_d, C_d);\n",
    "  cudaEventRecord(stop);\n",
    "\n",
    "  cudaEventSynchronize(stop);\n",
    "\n",
    "  float milliseconds = 0;\n",
    "  cudaEventElapsedTime(&milliseconds, start, stop);\n",
    "\n",
    "  cudaMemcpy(C, C_d, C_size, cudaMemcpyDeviceToHost);\n",
    "\n",
    "  double ops = 2.0 * atom_M * atom_N * atom_K; \n",
    "  double gflops = (ops / 1e9) / (milliseconds / 1000.0f);\n",
    "\n",
    "  printf(\"Time: %f ms\\n\", milliseconds);\n",
    "  printf(\"Performance: %f GFLOPS\\n\", gflops);\n",
    "  printf(\"Check C[0]: %f\\n\", C[0]);\n",
    "\n",
    "  cudaFreeHost(A); cudaFreeHost(B); cudaFreeHost(C);\n",
    "  cudaFree(A_d); cudaFree(B_d); cudaFree(C_d);\n",
    "\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "397cd4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M: 64, N: 128, K: 128\n",
      "Time: 0.0046 ms\n",
      "Performance: 455.1111 GFLOPS\n",
      "Max Error: 15.367403\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "#include <iostream>\n",
    "#include <vector>\n",
    "#include <random>\n",
    "#include <cmath>\n",
    "#include <cassert>\n",
    "#include <cstdio>\n",
    "#include <cuda_runtime.h>\n",
    "#include <cuda_bf16.h>\n",
    "#include <cudaTypedefs.h> \n",
    "#include <cuda.h>\n",
    "\n",
    "#define CUDA_CHECK(call) \\\n",
    "    do { \\\n",
    "        cudaError_t err = call; \\\n",
    "        if (err != cudaSuccess) { \\\n",
    "            fprintf(stderr, \"CUDA error at %s:%d code=%d(%s) \\\"%s\\\" \\n\", \\\n",
    "                __FILE__, __LINE__, err, cudaGetErrorString(err), #call); \\\n",
    "            exit(EXIT_FAILURE); \\\n",
    "        } \\\n",
    "    } while (0)\n",
    "\n",
    "PFN_cuTensorMapEncodeTiled_v12000 get_cuTensorMapEncodeTiled() {\n",
    "  cudaDriverEntryPointQueryResult driver_status;\n",
    "  void* cuTensorMapEncodeTiled_ptr = nullptr;\n",
    "  CUDA_CHECK(cudaGetDriverEntryPointByVersion(\"cuTensorMapEncodeTiled\", &cuTensorMapEncodeTiled_ptr, 12000, cudaEnableDefault, &driver_status));\n",
    "  assert(driver_status == cudaDriverEntryPointSuccess);\n",
    "  return reinterpret_cast<PFN_cuTensorMapEncodeTiled_v12000>(cuTensorMapEncodeTiled_ptr);\n",
    "}\n",
    "\n",
    "constexpr int N_warps = 32; \n",
    "constexpr int atom_M = 16; \n",
    "constexpr int atom_N = 16; \n",
    "constexpr int atom_K = 16;\n",
    "constexpr int tiled_M = 4;\n",
    "constexpr int tiled_N = 8;\n",
    "constexpr int tiled_K = 8;\n",
    "\n",
    "static_assert(tiled_M * tiled_N == N_warps, \"Error\");\n",
    "\n",
    "constexpr int M = atom_M * tiled_M; \n",
    "constexpr int N = atom_N * tiled_N; \n",
    "constexpr int K = atom_K * tiled_K; \n",
    "\n",
    "constexpr uint32_t rank = 2;\n",
    "uint64_t A_size[rank] = {K,M}; \n",
    "uint64_t B_size[rank] = {N,K}; \n",
    "uint64_t A_stride[rank-1] = {K*sizeof(__nv_bfloat16)}; \n",
    "uint64_t B_stride[rank-1] = {N*sizeof(__nv_bfloat16)};\n",
    "uint32_t A_smem_size[rank] = {atom_K, atom_M}; \n",
    "uint32_t B_smem_size[rank] = {atom_N, atom_K}; \n",
    "uint32_t elem_stride[rank] = {1,1};\n",
    "\n",
    "auto pfn_cuTensorMapEncodeTiled = get_cuTensorMapEncodeTiled();\n",
    "\n",
    "__global__ void wmma_ker(const __grid_constant__ CUtensorMap tensorMapA, \n",
    "                         const __grid_constant__ CUtensorMap tensorMapB, \n",
    "                         float *c) \n",
    "{\n",
    "    extern __shared__ uint8_t raw_smem[];\n",
    "}\n",
    "\n",
    "void cpu_matmul(const std::vector<float>& h_A, const std::vector<float>& h_B, std::vector<float>& h_C_ref) {\n",
    "    for (int i = 0; i < M; ++i) {\n",
    "        for (int j = 0; j < N; ++j) {\n",
    "            float sum = 0.0f;\n",
    "            for (int k = 0; k < K; ++k) {\n",
    "                sum += h_A[i * K + k] * h_B[k * N + j];\n",
    "            }\n",
    "            h_C_ref[i * N + j] = sum;\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "int main() \n",
    "{\n",
    "  size_t A_len = M * K;\n",
    "  size_t B_len = K * N;\n",
    "  size_t C_len = M * N;\n",
    "\n",
    "  size_t A_bytes = sizeof(__nv_bfloat16) * A_len;\n",
    "  size_t B_bytes = sizeof(__nv_bfloat16) * B_len;\n",
    "  size_t C_bytes = sizeof(float) * C_len;\n",
    "\n",
    "  std::vector<float> h_A_float(A_len);\n",
    "  std::vector<float> h_B_float(B_len);\n",
    "  std::vector<float> h_C_ref(C_len);\n",
    "  std::vector<float> h_C_gpu(C_len);\n",
    "\n",
    "  std::mt19937 gen(1337);\n",
    "  std::uniform_real_distribution<float> dis(-1.0f, 1.0f);\n",
    "\n",
    "  for(auto &x : h_A_float) x = dis(gen);\n",
    "  for(auto &x : h_B_float) x = dis(gen);\n",
    "\n",
    "  std::vector<__nv_bfloat16> h_A_bf16(A_len);\n",
    "  std::vector<__nv_bfloat16> h_B_bf16(B_len);\n",
    "\n",
    "  for(size_t i = 0; i < A_len; ++i) h_A_bf16[i] = __float2bfloat16(h_A_float[i]);\n",
    "  for(size_t i = 0; i < B_len; ++i) h_B_bf16[i] = __float2bfloat16(h_B_float[i]);\n",
    "\n",
    "  __nv_bfloat16 *d_A, *d_B;\n",
    "  float *d_C;\n",
    "  \n",
    "  cudaMalloc(&d_A, A_bytes);\n",
    "  cudaMalloc(&d_B, B_bytes);\n",
    "  cudaMalloc(&d_C, C_bytes);\n",
    "\n",
    "  cudaMemcpy(d_A, h_A_bf16.data(), A_bytes, cudaMemcpyHostToDevice);\n",
    "  cudaMemcpy(d_B, h_B_bf16.data(), B_bytes, cudaMemcpyHostToDevice);\n",
    "  cudaMemset(d_C, 0, C_bytes);\n",
    "\n",
    "  int threads_per_warp = 32;\n",
    "  int block_size = N_warps * threads_per_warp;\n",
    "  size_t smem_size = A_bytes + B_bytes + C_bytes;\n",
    "\n",
    "  cudaFuncSetAttribute(wmma_ker, cudaFuncAttributeMaxDynamicSharedMemorySize, smem_size);\n",
    "\n",
    "  CUtensorMap tMap_A{};\n",
    "  CUtensorMap tMap_B{}; \n",
    "  \n",
    "  CUresult res_A = pfn_cuTensorMapEncodeTiled(\n",
    "    &tMap_A, \n",
    "    CUtensorMapDataType::CU_TENSOR_MAP_DATA_TYPE_BFLOAT16,\n",
    "    rank,\n",
    "    d_A, \n",
    "    A_size,\n",
    "    A_stride,\n",
    "    A_smem_size,\n",
    "    elem_stride,\n",
    "    CUtensorMapInterleave::CU_TENSOR_MAP_INTERLEAVE_NONE,\n",
    "    CUtensorMapSwizzle::CU_TENSOR_MAP_SWIZZLE_NONE,\n",
    "    CUtensorMapL2promotion::CU_TENSOR_MAP_L2_PROMOTION_NONE,\n",
    "    CUtensorMapFloatOOBfill::CU_TENSOR_MAP_FLOAT_OOB_FILL_NONE\n",
    "  );\n",
    "  assert(res_A == CUDA_SUCCESS);\n",
    "  \n",
    "  CUresult res_B = pfn_cuTensorMapEncodeTiled(\n",
    "    &tMap_B, \n",
    "    CUtensorMapDataType::CU_TENSOR_MAP_DATA_TYPE_BFLOAT16,\n",
    "    rank,\n",
    "    d_B, \n",
    "    B_size,\n",
    "    B_stride,\n",
    "    B_smem_size,\n",
    "    elem_stride,\n",
    "    CUtensorMapInterleave::CU_TENSOR_MAP_INTERLEAVE_NONE,\n",
    "    CUtensorMapSwizzle::CU_TENSOR_MAP_SWIZZLE_NONE,\n",
    "    CUtensorMapL2promotion::CU_TENSOR_MAP_L2_PROMOTION_NONE,\n",
    "    CUtensorMapFloatOOBfill::CU_TENSOR_MAP_FLOAT_OOB_FILL_NONE\n",
    "  );\n",
    "  assert(res_B == CUDA_SUCCESS);\n",
    "\n",
    "  cudaEvent_t start, stop;\n",
    "  cudaEventCreate(&start);\n",
    "  cudaEventCreate(&stop);\n",
    "\n",
    "  wmma_ker<<<1, block_size, smem_size>>>(tMap_A, tMap_B, d_C);\n",
    "\n",
    "  cudaEventRecord(start);\n",
    "  wmma_ker<<<1, block_size, smem_size>>>(tMap_A, tMap_B, d_C);\n",
    "  cudaEventRecord(stop);\n",
    "\n",
    "  cudaEventSynchronize(stop);\n",
    "  float milliseconds = 0;\n",
    "  cudaEventElapsedTime(&milliseconds, start, stop);\n",
    "\n",
    "  cudaMemcpy(h_C_gpu.data(), d_C, C_bytes, cudaMemcpyDeviceToHost);\n",
    "\n",
    "  cpu_matmul(h_A_float, h_B_float, h_C_ref);\n",
    "\n",
    "  float max_error = 0.0f;\n",
    "  for(size_t i = 0; i < C_len; ++i) {\n",
    "      float diff = std::abs(h_C_ref[i] - h_C_gpu[i]);\n",
    "      if(diff > max_error) max_error = diff;\n",
    "  }\n",
    "\n",
    "  double flops = 2.0 * M * N * K;\n",
    "  double gflops = (flops * 1e-9) / (milliseconds / 1000.0);\n",
    "\n",
    "  printf(\"M: %d, N: %d, K: %d\\n\", M, N, K);\n",
    "  printf(\"Time: %.4f ms\\n\", milliseconds);\n",
    "  printf(\"Performance: %.4f GFLOPS\\n\", gflops);\n",
    "  printf(\"Max Error: %f\\n\", max_error);\n",
    "\n",
    "  cudaFree(d_A); cudaFree(d_B); cudaFree(d_C);\n",
    "  cudaEventDestroy(start); cudaEventDestroy(stop);\n",
    "\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd4751d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
