{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6dfb55c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source files will be saved in \"/tmp/tmpx4fljlmu\".\n"
     ]
    }
   ],
   "source": [
    "import cutlass\n",
    "import cutlass.cute as cute\n",
    "import cuda.bindings as cu\n",
    "%load_ext nvcc4jupyter\n",
    "\n",
    "from nvcc4jupyter import set_defaults\n",
    "set_defaults(compiler_args='-arch=sm_100a -Xptxas=-v -O0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12ae1586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Active Blocks per SM for 'myKernel': 9\n",
      "Number of SMs: 84\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%cuda\n",
    "#include<stdio.h>\n",
    "#include<cuda_runtime.h> \n",
    "#include<cuda.h> \n",
    "#include<mma.h> \n",
    "#include<cuda_bf16.h>\n",
    "#include <cuda/barrier>\n",
    "using barrier = cuda::barrier<cuda::thread_scope_block>;\n",
    "namespace cde = cuda::device::experimental;\n",
    "\n",
    "constexpr int M = 4096;\n",
    "constexpr int N = 4096;\n",
    "constexpr int K = 4096;\n",
    "constexpr int WGMMA_M = 16; \n",
    "constexpr int WGMMA_N = 16; \n",
    "constexpr int WGMMA_K = 16; \n",
    "constexpr int n_prod = 1; \n",
    "constexpr int n_cons = 4; \n",
    "constexpr int bM = 32; \n",
    "constexpr int bN = 32; \n",
    "constexpr int bK = 32;\n",
    "//each block will do a 32x64 matmul\n",
    "constexpr int cta_M = 32; \n",
    "constexpr int cta_N = 64;\n",
    "constexpr int cta_dim_M = M/cta_M;\n",
    "constexpr int cta_dim_N = N/cta_N;\n",
    "\n",
    "__global__ void matmul(__nv_bfloat16* A, __nv_bfloat16 *B, __nv_bfloat16 *C, \n",
    "                        const __grid_constant__ CUtensorMap tensor_map_A, const __grid_constant__ CUtensorMap tensor_map_B)\n",
    "{\n",
    "  __shared__ alignas(128) __nv_bfloat16 S0[2][(bM*bK) + (bK*bN)]; \n",
    "  __shared__ alignas(128) __nv_bfloat16 S1[2][(bM*bK) + (bK*bN)]; \n",
    "\n",
    "  int t = threadIdx.x; \n",
    "  int b = blockIdx.x; \n",
    "  int b_dim = blockDim.x; \n",
    "\n",
    "\n",
    "  __shared__ barrier S0_E, S0_F, S1_E, S1_F; \n",
    "  if (t == 0)\n",
    "  {\n",
    "    init(&S0_E, blockDim.x);\n",
    "    init(&S1_E, blockDim.x);\n",
    "    init(&S0_F, blockDim.x);\n",
    "    init(&S1_F, blockDim.x);\n",
    "    // Make initialized barrier visible in async proxy.\n",
    "    cde::fence_proxy_async_shared_cta();\n",
    "  }\n",
    "  __syncthreads(); \n",
    "  \n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "  int numBlocks;\n",
    "    int blockSize = 5*32; // Your specific block size\n",
    "    size_t dynamicSMemSize = 0; // Dynamic shared memory per block\n",
    "\n",
    "    // Calculates max blocks per SM based on registers/smem usage\n",
    "    cudaOccupancyMaxActiveBlocksPerMultiprocessor(\n",
    "        &numBlocks, \n",
    "        matmul, \n",
    "        blockSize, \n",
    "        dynamicSMemSize\n",
    "    );\n",
    "\n",
    "    printf(\"Max Active Blocks per SM for 'myKernel': %d\\n\", numBlocks);\n",
    "    int deviceId;\n",
    "    cudaGetDevice(&deviceId);\n",
    "\n",
    "    cudaDeviceProp prop;\n",
    "    cudaGetDeviceProperties(&prop, deviceId);\n",
    "\n",
    "    printf(\"Number of SMs: %d\\n\", prop.multiProcessorCount);\n",
    "    return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce580870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n"
     ]
    }
   ],
   "source": [
    "#so the data is with us we have 84 SMs, we will launch 5 warps, 1 producer, 4 consumer warps\n",
    "#so in one stage we can do 32x32 matmul, in two stages we can do a 32x64 matmul, and we would need \n",
    "print((4096//512)*(4096//512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fff8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
