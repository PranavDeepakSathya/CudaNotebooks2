{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ac1803b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from functools import partial\n",
    "from typing import List\n",
    "\n",
    "import cutlass\n",
    "import cutlass.cute as cute\n",
    "from cutlass.cute.runtime import from_dlpack\n",
    "from cutlass.cute import KeepPTX, KeepCUBIN\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0787cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cute.kernel#this is already gmem coalesced \n",
    "def naive_gemm(gA:cute.Tensor, gB:cute.Tensor, gC:cute.Tensor):\n",
    "  M,K = gA.shape\n",
    "  K,N = gB.shape\n",
    "  tid_x, tid_y, _ = cute.arch.thread_idx()\n",
    "  bid_x, bid_y, _ = cute.arch.block_idx()\n",
    "  bdim_x, bdim_y, _ = cute.arch.block_dim()\n",
    "  x = tid_x + (bid_x*bdim_x)\n",
    "  y = tid_y + (bid_y*bdim_y)\n",
    "  c_val = 0.0\n",
    "  if (x < N and y < M): \n",
    "    for k in range(K): \n",
    "      a_val = gA[y,k]\n",
    "      b_val = gB[k,x]\n",
    "      c_val += a_val*b_val\n",
    "    gC[y,x] = c_val\n",
    "      \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a654e4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cute.jit\n",
    "def naive_gemm_launcher(mA: cute.Tensor, mB: cute.Tensor, mC:cute.Tensor): \n",
    "  tpb_x = 32\n",
    "  tpb_y = 32 \n",
    "  M,K = mA.shape\n",
    "  K,N = mB.shape\n",
    "  cute.printf(mA.layout)\n",
    "  kernel = naive_gemm(mA, mB, mC)\n",
    "  kernel.launch(grid = (N//tpb_x, M//tpb_y,1), block=(tpb_x, tpb_y, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e2afe255",
   "metadata": {},
   "outputs": [],
   "source": [
    "M,K,N = 4096, 4096, 4096 \n",
    "a = torch.randn(M,K, device = \"cuda\", dtype=torch.float32)\n",
    "b = torch.randn(K,N, device = \"cuda\", dtype=torch.float32)\n",
    "c = torch.zeros(M,N, device = \"cuda\", dtype = torch.float32)\n",
    "\n",
    "a_ = from_dlpack(a)\n",
    "b_ = from_dlpack(b)\n",
    "c_ = from_dlpack(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42651e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4096,4096):(4096,1)\n"
     ]
    }
   ],
   "source": [
    "naive_gemm_compiled = cute.compile[KeepPTX](naive_gemm_launcher, a_, b_, c_)\n",
    "naive_gemm_compiled(a_,b_,c_)\n",
    "torch.testing.assert_close(c, torch.matmul(a, b), atol=1e-3, rtol=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a8b58f",
   "metadata": {},
   "source": [
    "# okay this is a beautiful verification as we can see when we use cute dl_pack, since torch \n",
    "# tensors are row major, our converted cute_layout is also row major. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "000e1363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark(callable, a_, b_, c_, M,N,K):\n",
    "    avg_time_us = cute.testing.benchmark(\n",
    "        callable,\n",
    "        kernel_arguments=cute.testing.JitArguments(a_, b_, c_),\n",
    "        warmup_iterations=5,\n",
    "        iterations=100,\n",
    "    )\n",
    "    \n",
    "    flop = 2*M*N*K\n",
    "    \n",
    "    giga_flop_per_second = flop/(avg_time_us*1000)\n",
    "    print(f\"giga_flops_per_second:{giga_flop_per_second}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "21da6473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "giga_flops_per_second:7801.126391856618\n"
     ]
    }
   ],
   "source": [
    "benchmark(naive_gemm_compiled, a_,b_,c_, M,N,K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac3d00b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
