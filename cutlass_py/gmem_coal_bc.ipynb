{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6bcd681",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from functools import partial\n",
    "from typing import List\n",
    "\n",
    "import cutlass\n",
    "import cutlass.cute as cute\n",
    "from cutlass.cute.runtime import from_dlpack\n",
    "from cutlass.cute import KeepPTX, KeepCUBIN\n",
    "import cutlass.cute.nvgpu as cgpu\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5acfb623",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cute.kernel\n",
    "def patterned_access(gA:cute.Tensor, gB:cute.Tensor):\n",
    "  tidx, _, _ = cute.arch.thread_idx()\n",
    "  bidx, _, _ = cute.arch.block_idx()\n",
    "  bdimx,_, _ = cute.arch.block_dim()\n",
    "  lane_id = tidx % 32 \n",
    "  warp_id = tidx // 32 \n",
    "\n",
    "  x = (((lane_id*7)+5)%32)*32 + (warp_id) + (bdimx*bidx)\n",
    "  gB[x] = gA[x]\n",
    "  \n",
    "@cute.kernel\n",
    "def coalesced_access(gA:cute.Tensor, gB:cute.Tensor):\n",
    "  tidx, _, _ = cute.arch.thread_idx()\n",
    "  bidx, _, _ = cute.arch.block_idx()\n",
    "  bdimx,_, _ = cute.arch.block_dim()\n",
    "  lane_id = tidx % 32 \n",
    "  warp_id = tidx // 32 \n",
    "  x = lane_id + (warp_id*32) + (bdimx*bidx)\n",
    "  gB[x] = gA[x]\n",
    "  \n",
    "\n",
    "@cute.kernel\n",
    "def vec_coal(gA, gB): \n",
    "  tidx, _, _ = cute.arch.thread_idx()\n",
    "  bidx, _, _ = cute.arch.block_idx()\n",
    "  bdimx,_, _ = cute.arch.block_dim()\n",
    "  inner_x = tidx + (bidx*bdimx) \n",
    "  a_val = gA[((None), (inner_x))].load()\n",
    "  gB[((None), (inner_x))] = a_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42352f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@cute.jit \n",
    "def coalesced_launcher(mA, mB): \n",
    "  N= mA.shape[0]\n",
    "  kernel = coalesced_access(mA,mB)\n",
    "  tpb = 1024\n",
    "  kernel.launch(grid=(N//tpb,1,1), block = (tpb,1,1))\n",
    "  \n",
    "@cute.jit \n",
    "def patterned_launcher(mA, mB): \n",
    "  N = mA.shape[0]\n",
    "  kernel = patterned_access(mA,mB)\n",
    "  tpb = 1024\n",
    "  kernel.launch(grid=(N//tpb,1,1), block = (tpb,1,1))\n",
    "  \n",
    "@cute.jit\n",
    "def vec_coal_launcher(mA,mB): \n",
    "  vec_shape = (4,)\n",
    "  gA = cute.zipped_divide(mA, vec_shape)\n",
    "  gB = cute.zipped_divide(mB, vec_shape)\n",
    "\n",
    "  N = gA.shape[1]\n",
    "  N_inner = N[0]\n",
    "  tpb = 1024\n",
    "  kernel = vec_coal(gA,gB)\n",
    "  kernel.launch(grid=(N_inner//tpb,1,1), block = (tpb,1,1))\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c40e649c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_elems = 4096*4096*16\n",
    "a,b = torch.randn(num_elems, device = \"cuda\", dtype=torch.float32) + 0.003, torch.randn(num_elems, device = \"cuda\", dtype=torch.float32) + 0.003\n",
    "c,d = torch.zeros(num_elems, device = \"cuda\", dtype = torch.float32), torch.zeros(num_elems, device = \"cuda\", dtype = torch.float32)\n",
    "e = torch.randn(num_elems, device = \"cuda\", dtype=torch.float32) + 0.004\n",
    "f = torch.zeros(num_elems, device = \"cuda\", dtype = torch.float32)\n",
    "a_ = from_dlpack(a, assumed_align=128)\n",
    "b_ = from_dlpack(b, assumed_align=128)\n",
    "c_ = from_dlpack(c, assumed_align=128)\n",
    "d_ = from_dlpack(d, assumed_align=128)\n",
    "e_ = from_dlpack(e, assumed_align=128)\n",
    "f_ = from_dlpack(f, assumed_align=128)\n",
    "patterned_compiled = cute.compile(patterned_launcher,a_,c_)\n",
    "coalesced_compiled = cute.compile(coalesced_launcher, b_,d_)\n",
    "vec_coal_compiled = cute.compile[KeepPTX](vec_coal_launcher, e_, f_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "26475326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mem_benchmark(callable, a_, b_, num_elements):\n",
    "    avg_time_us = cute.testing.benchmark(\n",
    "        callable,\n",
    "        kernel_arguments=cute.testing.JitArguments(a_, b_),\n",
    "        warmup_iterations=5,\n",
    "        iterations=200,\n",
    "    )\n",
    "\n",
    "    # Calculate metrics\n",
    "    # ----------------\n",
    "    dtype = a_.element_type\n",
    "\n",
    "    # Calculate total bytes transferred:\n",
    "    # - 2 reads (A and B) + 1 write (C)\n",
    "    # - Each element is dtype.width bits\n",
    "    bytes_per_element = dtype.width // 8\n",
    "    total_bytes = num_elements * bytes_per_element\n",
    "\n",
    "    # Calculate achieved bandwidth\n",
    "    achieved_bandwidth = (2*total_bytes) / (avg_time_us * 1000)  # GB/s\n",
    "\n",
    "    # Print results\n",
    "    # ------------\n",
    "    print(f\"Performance Metrics:\")\n",
    "    print(f\"-------------------\")\n",
    "    print(f\"Kernel execution time: {avg_time_us:.4f} us\")\n",
    "    print(f\"Memory throughput: {achieved_bandwidth:.2f} GB/s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197cd2dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mem_benchmark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmem_benchmark\u001b[49m(patterned_compiled, a_,c_, num_elems)\n\u001b[32m      2\u001b[39m mem_benchmark(coalesced_compiled, b_, d_, num_elems)\n\u001b[32m      3\u001b[39m mem_benchmark(vec_coal_compiled, e_, f_,num_elems)\n",
      "\u001b[31mNameError\u001b[39m: name 'mem_benchmark' is not defined"
     ]
    }
   ],
   "source": [
    "#Memory Bandwidth | 1,792 GB/s of 5090 and we're only about 200 GB/s off with just\n",
    "#vectorized access. \n",
    "mem_benchmark(patterned_compiled, a_,c_, num_elems)\n",
    "mem_benchmark(coalesced_compiled, b_, d_, num_elems)\n",
    "mem_benchmark(vec_coal_compiled, e_, f_,num_elems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d5cc327",
   "metadata": {},
   "outputs": [],
   "source": [
    "arch = cgpu.cpasync.Arch.sm_120a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "32dc7c53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Arch.sm_120a"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d01ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MmaF16BF16Op(ab_dtype=<class 'cutlass.base_dsl.typing.BFloat16'>, acc_dtype=<class 'cutlass.base_dsl.typing.Float32'>, shape_mnk=(16, 8, 16))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wmma = cgpu.warp.MmaF16BF16Op(cutlass.BFloat16, cutlass.Float32, (16,8,16))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09844095",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
